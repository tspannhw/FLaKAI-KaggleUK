{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1e6f53-5313-42c8-ad00-3cb5b9ffbceb",
   "metadata": {},
   "source": [
    "### Read and process VOD Clickstream Kaggle dataset from Netflix\n",
    "\n",
    "## First field is row ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba431603-d62d-44a1-a270-c8b4a5c3c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m16 packages\u001b[0m \u001b[2min 193ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m13 packages\u001b[0m \u001b[2min 772ms\u001b[0m\u001b[0m                                            \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m13 packages\u001b[0m \u001b[2min 23ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mconfluent-kafka\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.55.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.9.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2024.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mseaborn\u001b[0m\u001b[2m==0.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2024.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install pandas seaborn numpy matplotlib pillow confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e94139e3-c4ca-4080-a372-277d823d6346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 103ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 67ms\u001b[0m\u001b[0m                                               \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastavro\u001b[0m\u001b[2m==1.9.7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deaa366e-5e61-4227-a485-365dac32caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ebcc38-b96e-44ed-957a-c0edfc577355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>duration</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>release_date</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58773</td>\n",
       "      <td>2017-01-01 01:15:09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Angus, Thongs and Perfect Snogging</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>2008-07-25</td>\n",
       "      <td>26bd5987e8</td>\n",
       "      <td>1dea19f6fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58774</td>\n",
       "      <td>2017-01-01 13:56:02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The Curse of Sleeping Beauty</td>\n",
       "      <td>Fantasy, Horror, Mystery, Thriller</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>f26ed2675e</td>\n",
       "      <td>544dcbc510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58775</td>\n",
       "      <td>2017-01-01 15:17:47</td>\n",
       "      <td>10530.0</td>\n",
       "      <td>London Has Fallen</td>\n",
       "      <td>Action, Thriller</td>\n",
       "      <td>2016-03-04</td>\n",
       "      <td>f77e500e7a</td>\n",
       "      <td>7cbcc791bf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58776</td>\n",
       "      <td>2017-01-01 16:04:13</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Vendetta</td>\n",
       "      <td>Action, Drama</td>\n",
       "      <td>2015-06-12</td>\n",
       "      <td>c74aec7673</td>\n",
       "      <td>ebf43c36b6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58777</td>\n",
       "      <td>2017-01-01 19:16:37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The SpongeBob SquarePants Movie</td>\n",
       "      <td>Animation, Action, Adventure, Comedy, Family, ...</td>\n",
       "      <td>2004-11-19</td>\n",
       "      <td>a80d6fc2aa</td>\n",
       "      <td>a57c992287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id             datetime  duration                               title  \\\n",
       "0   58773  2017-01-01 01:15:09       0.0  Angus, Thongs and Perfect Snogging   \n",
       "1   58774  2017-01-01 13:56:02       0.0        The Curse of Sleeping Beauty   \n",
       "2   58775  2017-01-01 15:17:47   10530.0                   London Has Fallen   \n",
       "3   58776  2017-01-01 16:04:13      49.0                            Vendetta   \n",
       "4   58777  2017-01-01 19:16:37       0.0     The SpongeBob SquarePants Movie   \n",
       "\n",
       "                                              genres release_date    movie_id  \\\n",
       "0                             Comedy, Drama, Romance   2008-07-25  26bd5987e8   \n",
       "1                 Fantasy, Horror, Mystery, Thriller   2016-06-02  f26ed2675e   \n",
       "2                                   Action, Thriller   2016-03-04  f77e500e7a   \n",
       "3                                      Action, Drama   2015-06-12  c74aec7673   \n",
       "4  Animation, Action, Adventure, Comedy, Family, ...   2004-11-19  a80d6fc2aa   \n",
       "\n",
       "      user_id  \n",
       "0  1dea19f6fe  \n",
       "1  544dcbc510  \n",
       "2  7cbcc791bf  \n",
       "3  ebf43c36b6  \n",
       "4  a57c992287  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('vodclickstream_uk_movies_03.csv')\n",
    "# Rename columns in place\n",
    "# df.rename(columns={'Unnamed: 0': 'row_id'}, inplace=True)\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d2d518-e5b6-4360-9791-dd2e500a2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer, Consumer\n",
    "\n",
    "def read_config():\n",
    "  # reads the client configuration from client.properties\n",
    "  # and returns it as a key-value map\n",
    "  config = {}\n",
    "  with open(\"client.properties\") as fh:\n",
    "    for line in fh:\n",
    "      line = line.strip()\n",
    "      if len(line) != 0 and line[0] != \"#\":\n",
    "        parameter, value = line.strip().split('=', 1)\n",
    "        config[parameter] = value.strip()\n",
    "  return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983ffac6-758c-4d41-9e8c-800beae063a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\":\"record\",\"name\":\"netflixbehavior\",\"namespace\":\"com.netflix.audience\",\"doc\":\"kaggle uk netflix audience data.\",\"fields\":[{\"name\":\"row_id\",\"type\":\"int\",\"doc\":\"row id\"},{\"name\":\"datetime\",\"type\":[\"string\",\"null\"],\"doc\":\"datetime of the viewing\"},{\"name\":\"duration\",\"type\":[\"double\",\"null\"],\"doc\":\"how long was it watched?\"},{\"name\":\"title\",\"type\":\"string\",\"doc\":\"title of the video\"},{\"name\":\"genres\",\"type\":[\"string\",\"null\"],\"doc\":\"A list of comma-delimited movie/tv genres that apply to the title.\"},{\"name\":\"release_date\",\"type\":[\"string\",\"null\"],\"doc\":\"date of the titles release.\"},{\"name\":\"movie_id\",\"type\":\"string\",\"doc\":\"id of the movie.\"},{\"name\":\"user_id\",\"type\":\"string\",\"doc\":\"id of the user.\"}]}\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "config = read_config()\n",
    "\n",
    "schema_registry_client = SchemaRegistryClient({\n",
    "  'url': config['schema.registry.url'],\n",
    "  'basic.auth.user.info': '{}:{}'.format(config['schema.key'], config['schema.secret'])\n",
    "})\n",
    "\n",
    "subject_name = 'netflixbehavior-value'\n",
    "schema_str = schema_registry_client.get_latest_version(subject_name).schema.schema_str\n",
    "print(schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a062500-2d87-4fd0-8f54-60bd5684ad36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1732720738.003|GETSUBSCRIPTIONS|rdkafka#producer-6| [thrd:main]: Telemetry client instance id changed from AAAAAAAAAAAAAAAAAAAAAA to Ta1UqVU1Sha2lvVU9PSB5g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Error: Local: Queue full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1732720751.812|TERMINATE|rdkafka#producer-2| [thrd:app]: Producer terminating with 100000 messages (11521537 bytes) still in queue or transit: use flush() to wait for outstanding message delivery\n",
      "%4|1732720751.829|TERMINATE|rdkafka#producer-3| [thrd:app]: Producer terminating with 100000 messages (11521537 bytes) still in queue or transit: use flush() to wait for outstanding message delivery\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBufferError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 51\u001b[0m\n\u001b[1;32m     50\u001b[0m value \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m---> 51\u001b[0m \u001b[43mproducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnetflixbehavior\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrow_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m i \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.cache/uv/archive-v0/Rjfpw0X68245Vf7zGg2q3/lib/python3.11/site-packages/confluent_kafka/serializing_producer.py:142\u001b[0m, in \u001b[0;36mSerializingProducer.produce\u001b[0;34m(self, topic, key, value, partition, on_delivery, timestamp, headers)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ValueSerializationError(se)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSerializingProducer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mpartition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mon_delivery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_delivery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mBufferError\u001b[0m: Local: Queue full",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# if Error: Local: Queue full\u001b[39;00m\n\u001b[1;32m     60\u001b[0m producer\u001b[38;5;241m.\u001b[39mflush\n\u001b[0;32m---> 61\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     62\u001b[0m producer\u001b[38;5;241m.\u001b[39mpoll(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Producer terminating with 100000 messages (11521537 bytes) still in queue or transit: use flush() to wait for outstanding message delivery\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# https://developer.confluent.io/tutorials/optimize-producer-throughput/confluent.html\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from confluent_kafka import SerializingProducer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "config = read_config()\n",
    "\n",
    "schema_registry_client = SchemaRegistryClient({\n",
    "  'url': config['schema.registry.url'],\n",
    "  'basic.auth.user.info': '{}:{}'.format(config['schema.key'], config['schema.secret'])\n",
    "})\n",
    "\n",
    "subject_name = 'netflixbehavior-value'\n",
    "schema_str = schema_registry_client.get_latest_version(subject_name).schema.schema_str\n",
    "# print(schema_str)\n",
    "\n",
    "avro_serializer = AvroSerializer(schema_registry_client, schema_str)\n",
    "\n",
    "producer_conf = {\n",
    "    'bootstrap.servers': config['bootstrap.servers'],\n",
    "    'security.protocol': config['security.protocol'],\n",
    "    'sasl.mechanisms': config['sasl.mechanisms'],\n",
    "    'sasl.username': config['sasl.username'],\n",
    "    'sasl.password': config['sasl.password'],\n",
    "    'value.serializer': avro_serializer,\n",
    "    'key.serializer': StringSerializer(),\n",
    "    'batch.size': 200000,\n",
    "    'linger.ms': 100,\n",
    "    'compression.type': 'Lz4',\n",
    "    'acks': 1\n",
    "}\n",
    "\n",
    "#batch.size: increase to 100000–200000 (default 16384)\n",
    "#linger.ms: increase to 10–100 (default 0)\n",
    "#compression.type=lz4 (default none, i.e., no compression)\n",
    "#acks=1 (default all, since Apache Kafka version 3.0)\n",
    "\n",
    "producer = SerializingProducer(producer_conf)\n",
    "producer.flush\n",
    "\n",
    "i = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try: \n",
    "        value = row.to_dict()\n",
    "        producer.produce(topic='netflixbehavior', key=str(value['row_id']), value=value)\n",
    "        i = i + 1\n",
    "        if (i % 10000 == 0):\n",
    "            print(i)\n",
    "            producer.flush\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Error:\", ex)\n",
    "        # if Error: Local: Queue full\n",
    "        producer.flush\n",
    "        time.sleep(10)\n",
    "        producer.poll(0)\n",
    "        # Producer terminating with 100000 messages (11521537 bytes) still in queue or transit: use flush() to wait for outstanding message delivery\n",
    "        # https://developer.confluent.io/tutorials/optimize-producer-throughput/confluent.html\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Loaded rows time: {round(t1-t0, 4)} seconds\")\n",
    "print(\"Row Count\")\n",
    "print(len(df.index) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5853c-b1b2-4cd2-a16e-42581e6d2b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
